# 80/20 Minimum Viable Measurement Plan

This document describes the fastest, lowest-friction way to gather detailed data on how AI coding assistants are impacting developer workflows, without standing up a full analytics platform.

The plan was generated by GPT-5.

---

## 0. Core Signals (North Stars)

Track only four outcome-focused metrics:
- **Cycle time**: First commit → merge  
- **Review time**: PR opened → first review → merge  
- **Test coverage delta**: Coverage change per PR  
- **Defect rate**: Bug-fix PRs or issues linked to recent merges  

> Everything else (AI acceptance %, LOC, etc.) is diagnostic — not the goal.

---

## 1. Attribute AI Usage

**A. PR template checkboxes (self-reported; <10s per PR):**
- “AI used on this PR?” ☐ None ☐ A little (<25%) ☐ Moderate (25–75%) ☐ Heavily (>75%)  
- “AI used for (multi-select):” ☐ boilerplate ☐ tests ☐ refactor ☐ bugfix ☐ docs  

**B. Tool telemetry (if available):**
- Export org/team Copilot acceptance and active-user stats.  
- Require agents/MCP servers to open PRs (not push to main). The PR is your usage log.

---

## 2. Data Sources (No New Platform)

Use a small script or GH Action to assemble a weekly CSV per repo/team:

- **GitHub PR/API**: cycle time, review time, PR size, review count, labels, AI checkboxes.  
- **CI/Test**: JaCoCo/Sonar (or equivalent) → coverage delta per PR.  
- **Issues**: bug issues within 14–30 days of a merge, linked back to PRs.  

Store merged CSVs in repo (`/analytics/weekly/YYYY-WW.csv`) or shared drive.

---

## 3. Developer Pulse Survey (Monthly, 3 Minutes)

5 Likert questions (anonymous, per-team roll-up):

1. AI tools helped me complete work faster this month.  
2. I wrote better tests/caught edge cases thanks to AI.  
3. I needed more rework/bug fixes due to AI-generated code.  
4. My time in flow increased.  
5. Overall, AI made my job more enjoyable.  

---

## 4. Analysis Approach

- Compare cohorts by AI intensity (None / Low / Med / High).  
- Chart cycle time, review time, coverage delta, defect rate per cohort.  
- Look for directional deltas (e.g., High-AI PRs ship 18% faster with flat defects).  
- Use survey data to explain metrics (e.g., improved flow but coverage dropped).  

---

## 5. Lightweight Nudges

- **Tests when AI is heavy:** GH Action comments if PR marked “Heavily” but coverage delta ≤ 0.  
- **Review focus:** PR template adds checklist for reviewers on AI-heavy PRs: ✅ logic verified, ✅ security considered, ✅ edge cases tested.  

Reminders, not gates — aligned with lightweight culture.

---

## 6. Reporting (Monthly, One Page)

- **Org trend:** 4 north-star metrics.  
- **Cohorts:** None/Low/Med/High AI → % change vs baseline.  
- **Adoption:** % PRs with AI usage, % teams tagging PRs, survey results.  
- **Highlights:** 2 wins (e.g., faster cycle time), 1 risk (e.g., coverage dips).  

Distribute via Slack + short leadership slide.

---

## 7. Why This Works

- Zero new platform: GitHub + CI + one script.  
- High signal, low friction.  
- Trust-preserving: analysis at PR/team/cohort level, not individuals.  
- Actionable fast: within a month you’ll see where AI helps and where it hurts.

---

## Optional Starter Kit

- Add `PULL_REQUEST_TEMPLATE.md` with AI usage checkboxes.  
- Add GH Action: pulls PR metadata, merges CI coverage, outputs weekly CSV.  
- Post weekly summary in pinned Discussion.  
- Launch the 5-question monthly survey.  
- Provide a simple notebook or Google Sheet to chart outcomes by cohort.

---

**End of Plan**
