# Collecting Data on AI Coding Workflow Shifts

This document captures a discussion of to observe, interview, and analyze how AI coding assistants are changing developer workflows.

The plan was generated by GPT-5.

---

## 1. Define the Work Patterns You Want to Detect
Start by framing hypotheses about how AI might change workflows. For example:
- **Code writing**: More small commits vs. large ones? More boilerplate generated?  
- **Testing**: Are developers writing tests earlier/later? Are AI-generated tests common?  
- **Reviewing**: Do reviewers spend more time validating logic correctness than style?  
- **Deployment cadence**: Are releases faster or more frequent after AI adoption?  

Having these categories helps you focus data collection.

---

## 2. Structured Direct Observation / Shadowing
- Do **short observational studies**: watch developers for 30–60 minutes, noting how often they invoke AI, when they accept/reject suggestions, when they switch to testing, how often they context-switch, etc.  
- Use a **common observation template** so results are consistent (columns like: “Task type,” “AI invoked,” “Suggestion accepted,” “Tests written,” “Commit made,” “Pain points observed”).  
- Code the notes into a spreadsheet afterward so you can analyze frequencies and sequences.

---

## 3. Semi-Structured Interviews / Developer Diaries
- Ask developers to keep a **one-day activity log**: e.g. “When did AI help you? When did it slow you down? Did it change when/how you tested?”  
- In interviews, use consistent prompts (e.g., “Walk me through your last feature branch: where did AI speed things up, where did you distrust it?”).  
- Afterward, **tag interview notes** with categories (AI use for code, tests, refactoring, debugging, deployment prep) so you can see recurring patterns.

---

## 4. Lightweight Instrumentation
You don’t need a big analytics platform. A few simple hooks give you analyzable traces:
- **PR Template Checkboxes** (self-report per PR):  
  - “AI used: None / Low / Medium / High”  
  - “Used for: Boilerplate / Tests / Refactor / Docs / Bugfix”  
- **Commit Message Convention**: allow a `#ai` tag in commit messages when AI-generated code is included.  
- **CI Hooks**: capture test coverage delta and build/test success per PR.  

This way, each unit of work (commit/PR) carries an “AI fingerprint” you can correlate with workflow outcomes.

---

## 5. Make the Data Analyzable
- Combine observation logs, interviews, and PR/CI metadata into a **single coding scheme** (e.g., categories like “AI for test-writing” or “AI for refactoring”).  
- Use a simple spreadsheet or lightweight database to log events.  
- Then run basic counts and comparisons:  
  - % of PRs with AI used for tests vs. without  
  - Avg. cycle time for AI-heavy vs. AI-light PRs  
  - Interview themes that recur (e.g., “AI helps in boilerplate but hurts in debugging”).  

This will show *patterns of work*, not just outcomes.

---

## 6. Continuous Feedback Loop
- Share early findings with teams: “We’re seeing that AI is heavily used for test generation, but coverage still drops afterward.”  
- Ask them to validate or challenge your observations—this helps ensure the patterns are real and not artifacts of measurement.  
- Iterate: refine categories, adjust what you observe, and check again after a few weeks.

---

## Key Principle
Treat this like *qualitative research with lightweight quantification*.  
You’re not surveilling; you’re systematically tagging behavior so you can see which *new workflows* AI is introducing and then decide which ones to promote or correct.
